import streamlit as st
import os
import pickle
import faiss
import pytz
from datetime import datetime
import google.generativeai as genai
from sentence_transformers import SentenceTransformer, models

# --- Cáº¤U HÃŒNH TRANG ---
st.set_page_config(page_title="PhÃ²ng KhÃ¡m ÄÃ´ng Y AI", page_icon="ğŸ¥", layout="wide")

# --- KIá»‚M TRA ÄÆ¯á»œNG DáºªN Dá»® LIá»†U ---
POSSIBLE_PATHS = ['Saved_Model/Saved_Model', 'Saved_Model', '.']
def find_data_path():
    for path in POSSIBLE_PATHS:
        if os.path.exists(os.path.join(path, "my_faiss.index")):
            return path
    return None

DATA_PATH = find_data_path()

# --- Cáº¤U HÃŒNH API KEY (áº¨N) ---
# Code sáº½ tá»± Ä‘á»™ng láº¥y tá»« má»¥c Secrets cá»§a Streamlit Cloud
if "GEMINI_API_KEY" in st.secrets:
    API_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=API_KEY)
else:
    st.error("âŒ ChÆ°a cáº¥u hÃ¬nh API Key trong Secrets. Vui lÃ²ng kiá»ƒm tra cÃ i Ä‘áº·t trÃªn Streamlit Cloud.")
    st.stop()

# --- CÃC HÃ€M Há»– TRá»¢ ---
def get_vietnam_time():
    tz_VN = pytz.timezone('Asia/Ho_Chi_Minh')
    return datetime.now(tz_VN).strftime("%d/%m/%Y - %H:%M")

@st.cache_resource
def load_embedding_model():
    with st.spinner("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh ngÃ´n ngá»¯..."):
        try:
            word_embedding_model = models.Transformer('vinai/phobert-base', max_seq_length=256)
            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
            return SentenceTransformer(modules=[word_embedding_model, pooling_model])
        except:
            return SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')

@st.cache_resource
def load_rag_system(folder_path):
    if folder_path:
        index_path = os.path.join(folder_path, "my_faiss.index")
        chunks_path = os.path.join(folder_path, "chunks.pkl")
        try:
            index = faiss.read_index(index_path)
            with open(chunks_path, 'rb') as f:
                chunks = pickle.load(f)
            return index, chunks
        except Exception as e:
            st.error(f"Lá»—i khi Ä‘á»c file dá»¯ liá»‡u: {e}")
    return None, None

def retrieve_info(query, index, chunks, model, k=3):
    if index is None: return []
    q_emb = model.encode([query])[0].reshape(1, -1).astype('float32')
    _, indices = index.search(q_emb, k)
    return [chunks[i] for i in indices[0]]

def generate_consultation(query, book_knowledge, patient_history):
    # Sá»­ dá»¥ng tÃªn model á»•n Ä‘á»‹nh nháº¥t Ä‘á»ƒ trÃ¡nh lá»—i 404
    model = genai.GenerativeModel('gemini-2.5-flash') 
    
    prompt = f"""
    Báº¡n lÃ  má»™t BÃ¡c sÄ© Y há»c Cá»• truyá»n chuyÃªn nghiá»‡p. 
    Thá»i gian hiá»‡n táº¡i: {get_vietnam_time()}.
    Dá»® LIá»†U Tá»ª SÃCH: {book_knowledge}
    Lá»ŠCH Sá»¬ KHÃM: {patient_history}
    CÃ‚U Há»I Bá»†NH NHÃ‚N: {query}
    ---
    CHá»ˆ THá»Š Xá»¬ LÃ (TUÃ‚N THá»¦ NGHIÃŠM NGáº¶T):

    BÆ¯á»šC 1: KIá»‚M TRA Äá»˜ KHá»šP THÃ”NG TIN (QUAN TRá»ŒNG NHáº¤T)
    - HÃ£y Ä‘á»c ká»¹ pháº§n "KIáº¾N THá»¨C TRA Cá»¨U Tá»ª SÃCH" á»Ÿ trÃªn.
    - Náº¿u cÃ¢u há»i cá»§a bá»‡nh nhÃ¢n chá»©a cÃ¡c tá»« khÃ³a vá»:
        + Thuá»‘c TÃ¢y y (VÃ­ dá»¥: Panadol, Paracetamol, KhÃ¡ng sinh, Aspirin...).
        + Bá»‡nh danh hiá»‡n Ä‘áº¡i khÃ´ng cÃ³ trong ÄÃ´ng y (VÃ­ dá»¥: COVID-19, Ung thÆ° giai Ä‘oáº¡n cuá»‘i, Pháº«u thuáº­t, HIV...).
        + Hoáº·c ná»™i dung trong pháº§n "KIáº¾N THá»¨C TRA Cá»¨U" hoÃ n toÃ n khÃ´ng liÃªn quan Ä‘áº¿n cÃ¢u há»i.
    => HÃ€NH Äá»˜NG: Tráº£ lá»i ngáº¯n gá»n: "Xin lá»—i, há»‡ thá»‘ng dá»¯ liá»‡u Y há»c cá»• truyá»n hiá»‡n táº¡i khÃ´ng cÃ³ thÃ´ng tin vá» [TÃªn thuá»‘c/TÃªn bá»‡nh]. Vui lÃ²ng tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© chuyÃªn khoa."
    => TUYá»†T Äá»I KHÃ”NG cá»‘ gáº¯ng lÃ¡i sang tÆ° váº¥n triá»‡u chá»©ng.
    => TUYá»†T Äá»I KHÃ”NG Ä‘Æ°a ra lá»i khuyÃªn thay tháº¿. Dá»«ng cÃ¢u tráº£ lá»i táº¡i Ä‘Ã¢y.

    BÆ¯á»šC 2: Náº¾U THÃ”NG TIN Há»¢P Lá»† VÃ€ CÃ“ TRONG SÃCH ÄÃ”NG Y
    - Náº¿u bá»‡nh nhÃ¢n mÃ´ táº£ chung chung: HÃ£y há»i ngÆ°á»£c láº¡i (Váº¥n cháº©n) Ä‘á»ƒ lÃ m rÃµ thá»ƒ bá»‡nh (HÆ°/Thá»±c, HÃ n/Nhiá»‡t).
    - Náº¿u bá»‡nh nhÃ¢n mÃ´ táº£ rÃµ rÃ ng: ÄÆ°a ra cháº©n Ä‘oÃ¡n vÃ  bÃ i thuá»‘c dá»±a trÃªn "KIáº¾N THá»¨C TRA Cá»¨U".
    - Äá»ªNG Ä‘oÃ¡n mÃ².
    - HÃ£y Ä‘Ã³ng vai ngÆ°á»i dáº«n dáº¯t, Ä‘Æ°a ra 3-4 lá»±a chá»n tráº¯c nghiá»‡m dá»±a trÃªn cÃ¡c chá»©ng bá»‡nh trong sÃ¡ch Ä‘á»ƒ bá»‡nh nhÃ¢n chá»n.
    - VÃ­ dá»¥: "Äau bá»¥ng cÃ³ nhiá»u thá»ƒ. Báº¡n Ä‘au kiá»ƒu nÃ o? 1. Äau Ã¢m á»‰ (HÆ° hÃ n)? 2. Äau dá»¯ dá»™i (Thá»±c tÃ­ch)?"
    BÆ¯á»šC 3: TRÃCH DáºªN
    - Má»i lá»i khuyÃªn Ä‘Æ°a ra pháº£i dá»±a trÃªn sÃ¡ch.
    """
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"âŒ Lá»—i AI: {str(e)}"

# --- GIAO DIá»†N CHÃNH ---
with st.sidebar:
    st.header("âš™ï¸ Quáº£n lÃ½")
    patient_id = st.text_input("TÃªn bá»‡nh nhÃ¢n", value="KhÃ¡ch")
    if st.button("LÃ m má»›i cuá»™c há»™i thoáº¡i"):
        st.session_state.messages = []
        st.rerun()
    st.divider()
    if DATA_PATH:
        st.success("âœ… Há»‡ thá»‘ng Ä‘Ã£ káº¿t ná»‘i dá»¯ liá»‡u sÃ¡ch.")
    else:
        st.warning("âš ï¸ Äang cháº¡y khÃ´ng cÃ³ dá»¯ liá»‡u sÃ¡ch bá»• trá»£.")

if "messages" not in st.session_state:
    st.session_state.messages = []

embed_model = load_embedding_model()
faiss_index, all_chunks = load_rag_system(DATA_PATH)

st.title("ğŸ¥ PhÃ²ng KhÃ¡m ÄÃ´ng Y AI")
st.caption(f"Tráº¡ng thÃ¡i: Äang hoáº¡t Ä‘á»™ng | {get_vietnam_time()}")

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("MÃ´ táº£ triá»‡u chá»©ng cá»§a báº¡n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        context = ""
        if faiss_index:
            relevant = retrieve_info(prompt, faiss_index, all_chunks, embed_model)
            context = "\n".join(relevant)
        
        history = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-3:]])
        
        with st.spinner("BÃ¡c sÄ© Ä‘ang xem há»“ sÆ¡..."):
            response = generate_consultation(prompt, context, history)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
