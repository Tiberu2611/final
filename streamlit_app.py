import streamlit as st
import os
import pickle
import faiss
import pytz
from datetime import datetime
import google.generativeai as genai
from sentence_transformers import SentenceTransformer, models

# --- Cáº¤U HÃŒNH TRANG ---
st.set_page_config(page_title="PhÃ²ng KhÃ¡m ÄÃ´ng Y AI", page_icon="ğŸ¥", layout="wide")

# --- KIá»‚M TRA ÄÆ¯á»œNG DáºªN Dá»® LIá»†U ---
POSSIBLE_PATHS = ['Saved_Model/Saved_Model', 'Saved_Model', '.']
def find_data_path():
    for path in POSSIBLE_PATHS:
        if os.path.exists(os.path.join(path, "my_faiss.index")):
            return path
    return None

DATA_PATH = find_data_path()

# --- Cáº¤U HÃŒNH API KEY (áº¨N) ---
# Code sáº½ tá»± Ä‘á»™ng láº¥y tá»« má»¥c Secrets cá»§a Streamlit Cloud
if "GEMINI_API_KEY" in st.secrets:
    API_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=API_KEY)
else:
    st.error("âŒ ChÆ°a cáº¥u hÃ¬nh API Key trong Secrets. Vui lÃ²ng kiá»ƒm tra cÃ i Ä‘áº·t trÃªn Streamlit Cloud.")
    st.stop()

# --- CÃC HÃ€M Há»– TRá»¢ ---
def get_vietnam_time():
    tz_VN = pytz.timezone('Asia/Ho_Chi_Minh')
    return datetime.now(tz_VN).strftime("%d/%m/%Y - %H:%M")

@st.cache_resource
def load_embedding_model():
    with st.spinner("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh ngÃ´n ngá»¯..."):
        try:
            word_embedding_model = models.Transformer('vinai/phobert-base', max_seq_length=256)
            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
            return SentenceTransformer(modules=[word_embedding_model, pooling_model])
        except:
            return SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')

@st.cache_resource
def load_rag_system(folder_path):
    if folder_path:
        index_path = os.path.join(folder_path, "my_faiss.index")
        chunks_path = os.path.join(folder_path, "chunks.pkl")
        try:
            index = faiss.read_index(index_path)
            with open(chunks_path, 'rb') as f:
                chunks = pickle.load(f)
            return index, chunks
        except Exception as e:
            st.error(f"Lá»—i khi Ä‘á»c file dá»¯ liá»‡u: {e}")
    return None, None

def retrieve_info(query, index, chunks, model, k=3):
    if index is None: return []
    q_emb = model.encode([query])[0].reshape(1, -1).astype('float32')
    _, indices = index.search(q_emb, k)
    return [chunks[i] for i in indices[0]]

def generate_consultation(query, book_knowledge, patient_history):
    # Sá»­ dá»¥ng tÃªn model á»•n Ä‘á»‹nh nháº¥t Ä‘á»ƒ trÃ¡nh lá»—i 404
    model = genai.GenerativeModel('gemini-2.5-flash') 
    
    prompt = f"""
    Báº¡n lÃ  má»™t BÃ¡c sÄ© Y há»c Cá»• truyá»n chuyÃªn nghiá»‡p. 
    Thá»i gian hiá»‡n táº¡i: {get_vietnam_time()}.
    Dá»® LIá»†U Tá»ª SÃCH: {book_knowledge}
    Lá»ŠCH Sá»¬ KHÃM: {patient_history}
    CÃ‚U Há»I Bá»†NH NHÃ‚N: {query}
    YÃŠU Cáº¦U: Pháº£n há»“i Ã¢n cáº§n, chuyÃªn mÃ´n. Náº¿u cÃ¢u há»i chung chung, hÃ£y Ä‘Æ°a ra gá»£i Ã½ tráº¯c nghiá»‡m.
    """
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"âŒ Lá»—i AI: {str(e)}"

# --- GIAO DIá»†N CHÃNH ---
with st.sidebar:
    st.header("âš™ï¸ Quáº£n lÃ½")
    patient_id = st.text_input("TÃªn bá»‡nh nhÃ¢n", value="KhÃ¡ch")
    if st.button("LÃ m má»›i cuá»™c há»™i thoáº¡i"):
        st.session_state.messages = []
        st.rerun()
    st.divider()
    if DATA_PATH:
        st.success("âœ… Há»‡ thá»‘ng Ä‘Ã£ káº¿t ná»‘i dá»¯ liá»‡u sÃ¡ch.")
    else:
        st.warning("âš ï¸ Äang cháº¡y khÃ´ng cÃ³ dá»¯ liá»‡u sÃ¡ch bá»• trá»£.")

if "messages" not in st.session_state:
    st.session_state.messages = []

embed_model = load_embedding_model()
faiss_index, all_chunks = load_rag_system(DATA_PATH)

st.title("ğŸ¥ PhÃ²ng KhÃ¡m ÄÃ´ng Y AI")
st.caption(f"Tráº¡ng thÃ¡i: Äang hoáº¡t Ä‘á»™ng | {get_vietnam_time()}")

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("MÃ´ táº£ triá»‡u chá»©ng cá»§a báº¡n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        context = ""
        if faiss_index:
            relevant = retrieve_info(prompt, faiss_index, all_chunks, embed_model)
            context = "\n".join(relevant)
        
        history = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-3:]])
        
        with st.spinner("BÃ¡c sÄ© Ä‘ang xem há»“ sÆ¡..."):
            response = generate_consultation(prompt, context, history)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
